{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Xinyao Niu\n",
    "###### Python version: 3.6.8\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "# version of python\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from collections import defaultdict as dd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_call(c:[str]) -> None:\n",
    "    res = subprocess.check_output(c)\n",
    "    ls = []\n",
    "    for line in res.splitlines():\n",
    "        line = line.decode('utf-8')\n",
    "        if (line.split('.')[-1] != \"txt\") and (line != \"test.csv\"):\n",
    "            ls.append(line)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anneal.csv', 'breast-cancer.csv', 'car.csv', 'cmc.csv', 'hepatitis.csv', 'hypothyroid.csv', 'mushroom.csv', 'nursery.csv', 'primary-tumor.csv']\n"
     ]
    }
   ],
   "source": [
    "datapath = join(os.getcwd(),'2019S1-proj1-data')\n",
    "files = bash_call(['ls', datapath])\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xinyaoniu/Documents/COMP30027-ML/prject 1/2019S1-proj1-data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "        \"anneal.csv\": \"family,product-type,steel,carbon,hardness,temper_rolling,condition,formability,strength,non-ageing,surface-finish,surface-quality,enamelability,bc,bf,bt,bw-me,bl,m,chrom,phos,cbond,marvi,exptl,ferro,corr,bbvc,lustre,jurofm,s,p,shape,oil,bore,packing,class\".split(\",\"),\n",
    "        \"breast-cancer.csv\": \"age,menopause,tumor-size,inv-nodes,node-caps,deg-malig,breast,breast-quad,irradiat,class\".split(\",\"),\n",
    "        \"car.csv\": \"buying,maint,doors,persons,lug_boot,safety,class\".split(\",\"),\n",
    "        \"cmc.csv\": \"w-education,h-education,n-child,w-relation,w-work,h-occupation,standard-of-living,media-exposure,class\".split(\",\"),\n",
    "        \"hepatitis.csv\": \"sex,steroid,antivirals,fatigue,malaise,anorexia,liver-big,liver-firm,spleen-palpable,spiders,ascites,varices,histology,class\".split(\",\"),\n",
    "        \"hypothyroid.csv\": \"sex,on-thyroxine,query-on-thyroxine,on_antithyroid,surgery,query-hypothyroid,query-hyperthyroid,pregnant,sick,tumor,lithium,goitre,TSH,T3,TT4,T4U,FTI,TBG,class\".split(\",\"),\n",
    "        \"mushroom.csv\": \"cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat,class\".split(\",\"),\n",
    "        \"nursery.csv\": \"parents,has_nurs,form,children,housing,finance,social,health,class\".split(\",\"),\n",
    "        \"primary-tumor.csv\": \"age,sex,histologic-type,degree-of-diffe,bone,bone-marrow,lung,pleura,peritoneum,liver,brain,skin,neck,supraclavicular,axillar,mediastinum,abdominal,class\".split(\",\"),\n",
    "        \"test.csv\":[\"0\",\"1\",\"2\",\"3\",\"4\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption that I made for this project (details about how to process refer to code comments)\n",
    "- For better performance, missing values has been processed. \n",
    "- Using epsilon smoothing as a default smoothing method, also provide entrence for laplace smoothing.\n",
    "- all of the acc calculated are based on above assumptions\n",
    "- When calculating IG, missing values were not counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes_learner():\n",
    "    \n",
    "    def __init__(self, path:str, file:str):\n",
    "        self.file = file\n",
    "        self.path = join(path, file)\n",
    "        self.df = None  # primary dataframe\n",
    "        self.labels = []  # possible labels for the class\n",
    "        self.attributes = []  # possible attributes for each row\n",
    "        self.last = -1  # index of the labels\n",
    "        \n",
    "        self.prob = []  # the model hyper-parameters\n",
    "        self.missing = []  # number of missing value in each attributes\n",
    "        self.label_prob = []  # the probability for the prior\n",
    "        self.test_size = -1\n",
    "        self.train_size = -1\n",
    "        self.train_set = pd.DataFrame()\n",
    "        self.test_set = pd.DataFrame()\n",
    "        \n",
    "        self.predictions = []  # contains the prediction results in a form of (prediction, real labels)\n",
    "        \n",
    "        self.acc = 0  # accuracy of the predictions\n",
    "        \n",
    "        self.ig = {}\n",
    "        \n",
    "        \n",
    "    def preprocess(self, val:str='hold-out', shuffle:bool=False, hold:float=0.2) -> None:\n",
    "        \"\"\"\n",
    "        open a csv and transform it into a usable format\n",
    "        \n",
    "        *val -- define the evaluation method, it use hold out in defualt\n",
    "        \n",
    "        *shuffle -- whether we want to shuffle the dataset, it set to false in defaut\n",
    "\n",
    "        *hold -- if the val method is hold-out, this parameter can change the proportion between\n",
    "        training set and test set, it set to 0.2 in default\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.path, header=None)\n",
    "        self.df.columns = HEADERS[self.file]\n",
    "        self.size = len(self.df.index)\n",
    "        # labels location\n",
    "        self.last = self.df.columns[-1]\n",
    "        # remove duplicate labels\n",
    "        self.labels = list(set(self.df[self.last]))\n",
    "        \n",
    "        if shuffle or val==\"random sampling\":\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        if((val==\"random sampling\") and (hold != 0)):\n",
    "            self.test_size = int(self.df.shape[0] * hold)\n",
    "            self.train_size = self.df.shape[0] - self.test_size\n",
    "\n",
    "            self.train_set = self.df.head(self.train_size)\n",
    "            self.test_set = self.df.tail(self.test_size)\n",
    "        elif ((val==\"hold-out\") and (hold != 0)):\n",
    "            self.train_set = pd.DataFrame.copy(self.df)\n",
    "            for l in self.labels:\n",
    "                # randomly sampling out test set from the real data set\n",
    "                select = self.train_set[self.train_set[self.last] == l].sample(frac=hold)\n",
    "                # drop the testing set\n",
    "                self.train_set = self.train_set.drop(select.index, axis=0)\n",
    "                self.test_set = self.test_set.append(select)\n",
    "\n",
    "            # reset the indexes\n",
    "            self.train_set = self.train_set.reset_index(drop=True)\n",
    "            self.test_set = self.test_set.reset_index(drop=True)\n",
    "            \n",
    "            self.test_size = self.df.shape[0]\n",
    "            self.train_size = self.df.shape[0]\n",
    "        else:\n",
    "            self.train_set = self.df\n",
    "            self.test_set = self.df\n",
    "            self.test_size = self.df.shape[0]\n",
    "            self.train_size = self.df.shape[0]\n",
    "        \n",
    "        #print(\"train=\", self.train_set.shape, \"test=\", self.test_set.shape)\n",
    "\n",
    "        # pre-process the train set\n",
    "        for i in self.train_set.columns:\n",
    "            stats = self.count_frequency(self.train_set[[i,self.last]])\n",
    "            self.attributes.append(tuple(set(self.train_set[i])))\n",
    "            self.prob.append(stats)\n",
    "            #print(\"len of prob =\", len(self.prob))\n",
    "        #print(\"len of header =\", len(self.train_set.columns))\n",
    "            \n",
    "    \n",
    "    def count_frequency(self, df:pd.DataFrame ,accumulate:dict={}, laplace:bool=False) -> dd:\n",
    "        \"\"\"\n",
    "        helper function for preprocess\n",
    "        counting the frequency for each different value in an array\n",
    "        \n",
    "        `df` -- input dataframe that need to be processed, normally contains two columns\n",
    "        and one of it is labels\n",
    "        \n",
    "        `accumulate` -- the count number that might add on the top of the current on, it\n",
    "        is empty by default\n",
    "        \n",
    "        `laplace` -- whether we are going to use laplace smoothing or not\n",
    "        \"\"\"\n",
    "        \n",
    "        if(laplace):\n",
    "            res = dd(lambda:1, accumulate)\n",
    "        else:\n",
    "            res = dd(lambda:0,accumulate)\n",
    "            \n",
    "        miss = dd(lambda:0)\n",
    "        \n",
    "        index = df.columns\n",
    "        same = False\n",
    "        \n",
    "        # rename the index if they are the same\n",
    "        if index[0] == index[1]:\n",
    "            index = ['one', 'two']\n",
    "            df.columns = index\n",
    "            same = True\n",
    "            \n",
    "        for i in range(df.shape[0]):\n",
    "            feature = df[index[0]].loc[i]\n",
    "            label = df[index[1]].loc[i]\n",
    "            if same:\n",
    "                res[str(label)] += 1\n",
    "            elif str(feature) == '?': \n",
    "                #ignore the missing value\n",
    "                miss[str(label)] += 1\n",
    "            else:\n",
    "                res[str(feature)+'|'+ str(label)] += 1\n",
    "        \n",
    "        self.missing.append(miss)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def find_condition(self, prob:str) -> str:\n",
    "        \"\"\"\n",
    "        Helper function of train\n",
    "        finding the condition of a conditional probability\n",
    "        \n",
    "        `prob` -- the conditional probability\n",
    "        \"\"\"\n",
    "        return prob.split(\"|\")[1]\n",
    "    \n",
    "    \n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculating the conditional probability for NB\n",
    "        \"\"\"\n",
    "        label = self.prob[-1]\n",
    "        #print(label)\n",
    "        for i in range(len(self.prob)-1):\n",
    "            current = self.prob[i]\n",
    "            missing = self.missing[i]\n",
    "            #print(current)\n",
    "            for k,v in current.items():\n",
    "                \n",
    "                real = label[self.find_condition(k)] - missing[self.find_condition(k)]\n",
    "                \n",
    "                #if(k==\"2|A\"):\n",
    "                    #print(\"2|A\", current[k],label[self.find_condition(k)], missing[self.find_condition(k)], real)\n",
    "                    \n",
    "                if(real != 0):\n",
    "                    current[k] /= real\n",
    "                else:\n",
    "                    current[k] = 0\n",
    "                    \n",
    "        for k,v in label.items():\n",
    "            label[k] /= self.train_size\n",
    "        \n",
    "        self.prob[-1] = label\n",
    "    \n",
    "    \n",
    "    def predict(self) -> None:\n",
    "        \"\"\"\n",
    "        predicting the class for an instance or a set of instances, basd on a trained model\n",
    "        \"\"\"\n",
    "        \n",
    "        features = self.test_set[self.test_set.columns[:-1]]\n",
    "        results = self.test_set[self.test_set.columns[-1]]\n",
    "        \n",
    "        for i in features.index:\n",
    "            p = np.zeros(len(self.labels))\n",
    "            for l in range(len(self.labels)):\n",
    "                p[l] = (self.prob_calculator(features.loc[i], self.labels[l]))\n",
    "            #print(features.loc[i])\n",
    "            #print(self.labels)\n",
    "            #print(p, self.labels[np.argmax(p)], results.loc[i])\n",
    "            self.predictions.append((self.labels[np.argmax(p)], results.loc[i]))\n",
    "    \n",
    "    \n",
    "    def prob_calculator(self, features:pd.DataFrame, target:str) -> float:\n",
    "        \"\"\"\n",
    "        helper function of predict\n",
    "        calculate the probability based on the given features\n",
    "        \n",
    "        `features` -- dataframe that needs to calculate the probability\n",
    "        \n",
    "        `target` -- given label\n",
    "        \"\"\"\n",
    "        \n",
    "        # epsilon for epsilon smoothing\n",
    "        eps = 0.0001\n",
    "        \n",
    "        if(self.prob[-1][target] != 0):\n",
    "            res = log(self.prob[-1][target],2)\n",
    "        else:\n",
    "            res = log(eps,2)\n",
    "        n = 0\n",
    "        for f in features:\n",
    "            if str(f) != '?':\n",
    "                condition = str(f) + '|' + target\n",
    "                # using epsilon smoothing in default\n",
    "                if (self.prob[n][condition] <= 0):\n",
    "                    res += log(eps,2)\n",
    "                else:\n",
    "                    res += log(self.prob[n][condition],2)\n",
    "            n += 1\n",
    "        return res\n",
    "\n",
    "\n",
    "    def evaluate(self, acc_v:bool) -> None:\n",
    "        \"\"\"\n",
    "        evaluate a set of predictions, in a supervised context\n",
    "        \n",
    "        `acc_v` -- make it True if you want to see the acc output\n",
    "        \"\"\"\n",
    "        #print(self.predictions)\n",
    "        total = len(self.predictions)\n",
    "        correct = 0\n",
    "        \n",
    "        for pair in self.predictions:\n",
    "            if pair[0] == pair[1]:\n",
    "                correct += 1\n",
    "                \n",
    "        self.acc = correct/total\n",
    "        \n",
    "        if acc_v:\n",
    "            print(\"correct =\", correct, \"total =\",total)\n",
    "            print(\"acc =\",correct/total)\n",
    "        \n",
    "    \n",
    "    def info_gain(self) -> None:\n",
    "        \"\"\"\n",
    "        calculate the information gain of an attributes with respect to the labels\n",
    "        \"\"\"\n",
    "        full = []  # contains the frequency of each atttribute along their column\n",
    "        for i in self.test_set:\n",
    "            full.append(self.count_frequency(self.df[[i,self.last]]))\n",
    "        \n",
    "        parent = 0\n",
    "        n = self.df.shape[0]\n",
    "        \n",
    "        for i in full[-1].values():\n",
    "            parent += self.entropy(i/n)\n",
    "        #print(\"parent=\",parent,\"n=\", n)\n",
    "        #print(full[-1])\n",
    "        \n",
    "        for i in range(len(full)-1):\n",
    "            current = full[i]\n",
    "            child = 0\n",
    "            for attr in self.attributes[i]:\n",
    "                a = []\n",
    "                for l in self.labels:\n",
    "                    # ignore the missing value\n",
    "                    if attr != \"?\":\n",
    "                        target = str(attr) + \"|\" + l\n",
    "                        a.append(current[target])\n",
    "                # weighted entropy for this child node\n",
    "                child += (sum(a)/self.size)*sum([self.entropy(i/sum(a)) for i in a])\n",
    "            self.ig[self.df.columns[i]] = parent-child\n",
    "        \n",
    "        \n",
    "    def entropy(self, p:float) -> float:\n",
    "        \"\"\"\n",
    "        helper function for info_gain\n",
    "        mainly for calculate the entropy for given probability\n",
    "        \n",
    "        *p -- given probability\n",
    "        \"\"\"\n",
    "        if(p==0):\n",
    "            return 0\n",
    "        return -p*log(p,2)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run(self, v=\"hold-out\", h=0.2, s=False, l=False, ig=False, mi=False, acc_v=False) -> None:\n",
    "        \"\"\"\n",
    "        this function will automatically run through the workflow with different\n",
    "        set up parameters. It is only for convenient testing and debuging purpose.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.preprocess(shuffle=s, val=v, hold=h)\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        self.predict()\n",
    "        self.evaluate(acc_v)\n",
    "        \n",
    "        if mi:\n",
    "            print(\"missing value for each column\", [sum(i.values()) for i in self.missing])\n",
    "            print(\"number of possible labels =\",len(self.labels))\n",
    "        \n",
    "        if ig:\n",
    "            self.info_gain()\n",
    "            print(\"\\nIG *\")\n",
    "            print(self.ig)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing file: anneal.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 5\n",
      "\n",
      "IG *\n",
      "{'family': 0.40908953764451006, 'product-type': 0.0, 'steel': 0.3060515354289406, 'carbon': 0.051344088764404106, 'hardness': 0.29108220585994704, 'temper_rolling': 0.14711886228095605, 'condition': 0.2137228803159088, 'formability': 0.29223544065798446, 'strength': 0.1261663361036096, 'non-ageing': 0.14107379163812883, 'surface-finish': 0.032488406491841815, 'surface-quality': 0.43517783626288564, 'enamelability': 0.03870173274881061, 'bc': 0.0004376065202120749, 'bf': 0.03935557414283686, 'bt': 0.021775078259213876, 'bw-me': 0.037997478813511565, 'bl': 0.036703081364408474, 'm': 0.0, 'chrom': 0.11722522630372034, 'phos': 0.029753745208638938, 'cbond': 0.02704235332867677, 'marvi': 0.0, 'exptl': 0.015604780443500665, 'ferro': 0.13718113252042574, 'corr': 0.0, 'bbvc': 0.0223970898516459, 'lustre': 0.01824168402125048, 'jurofm': 0.0, 's': 0.0, 'p': 0.0, 'shape': 0.04323960556514961, 'oil': 0.03303757117705697, 'bore': 0.01937886432831948, 'packing': 0.003958783545891853}\n",
      "=============================================\n",
      "now processing file: breast-cancer.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 0, 0, 0, 8, 0, 0, 1, 0, 0]\n",
      "number of possible labels = 2\n",
      "\n",
      "IG *\n",
      "{'age': 0.010605956535614136, 'menopause': 0.0020016149737116518, 'tumor-size': 0.0571711253242968, 'inv-nodes': 0.06899508808988597, 'node-caps': 0.08012009687900967, 'deg-malig': 0.07700985251661441, 'breast': 0.0024889884332655043, 'breast-quad': 0.01506662205414988, 'irradiat': 0.025819023909141148}\n",
      "=============================================\n",
      "now processing file: car.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 4\n",
      "\n",
      "IG *\n",
      "{'buying': 0.09644896916961399, 'maint': 0.07370394692148596, 'doors': 0.00448571662663233, 'persons': 0.2196629633399082, 'lug_boot': 0.030008141247605424, 'safety': 0.262184356554264}\n",
      "=============================================\n",
      "now processing file: cmc.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 3\n",
      "\n",
      "IG *\n",
      "{'w-education': 0.07090633894894594, 'h-education': 0.0401385992293839, 'n-child': 0.10173991727554088, 'w-relation': 0.009820501434385065, 'w-work': 0.002582332379721608, 'h-occupation': 0.030474214560266555, 'standard-of-living': 0.032511460053806784, 'media-exposure': 0.01578645559562042}\n",
      "=============================================\n",
      "now processing file: hepatitis.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 1, 0, 1, 1, 1, 10, 11, 5, 5, 5, 5, 0, 0]\n",
      "number of possible labels = 2\n",
      "\n",
      "IG *\n",
      "{'sex': 0.03660746514280977, 'steroid': 0.015265380561918285, 'antivirals': 0.014490701150154384, 'fatigue': 0.08645063847884216, 'malaise': 0.08322845589007444, 'anorexia': 0.013806029835453981, 'liver-big': 0.0903522944652434, 'liver-firm': 0.09049078626122975, 'spleen-palpable': 0.058739302370822144, 'spiders': 0.12938741279822152, 'ascites': 0.15163520023638288, 'varices': 0.10012174391687245, 'histology': 0.08493296456638777}\n",
      "=============================================\n",
      "now processing file: hypothyroid.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 2\n",
      "\n",
      "IG *\n",
      "{'sex': 0.004628873031652547, 'on-thyroxine': 0.0009139351160850073, 'query-on-thyroxine': 0.0012382074503017315, 'on_antithyroid': 0.00014844815831743796, 'surgery': 0.0009985293906336068, 'query-hypothyroid': 0.0013683791752741592, 'query-hyperthyroid': 0.0005423006444424394, 'pregnant': 0.0004350938464638965, 'sick': 0.0004888757691284829, 'tumor': 0.0008983004044028076, 'lithium': 4.463778824304043e-05, 'goitre': 7.8684698479492e-05, 'TSH': 0.009353710215580346, 'T3': 0.004075493419623766, 'TT4': 0.005792553705846859, 'T4U': 0.005768288201614624, 'FTI': 0.005744031245602799, 'TBG': 0.002580427555574416}\n",
      "=============================================\n",
      "now processing file: mushroom.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2480, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 2\n",
      "\n",
      "IG *\n",
      "{'cap-shape': 0.04879670193537311, 'cap-surface': 0.028590232773772817, 'cap-color': 0.03604928297620402, 'bruises': 0.19237948576121966, 'odor': 0.9060749773839998, 'gill-attachment': 0.014165027250616302, 'gill-spacing': 0.10088318399657026, 'gill-size': 0.23015437514804615, 'gill-color': 0.41697752341613126, 'stalk-shape': 0.007516772569664321, 'stalk-root': 0.4001378247172982, 'stalk-surface-above-ring': 0.2847255992184845, 'stalk-surface-below-ring': 0.2718944733927464, 'stalk-color-above-ring': 0.2538451734622399, 'stalk-color-below-ring': 0.24141556652756657, 'veil-type': 0.0, 'veil-color': 0.0238170161209168, 'ring-number': 0.03845266924309054, 'ring-type': 0.3180215107935376, 'spore-print-color': 0.4807049176849154, 'population': 0.2019580190668524, 'habitat': 0.156833604605092}\n",
      "=============================================\n",
      "now processing file: nursery.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 5\n",
      "\n",
      "IG *\n",
      "{'parents': 0.07293460750309966, 'has_nurs': 0.1964492804881155, 'form': 0.005572591715219399, 'children': 0.011886431475775616, 'housing': 0.019602025022871672, 'finance': 0.0043331270252000564, 'social': 0.022232616894018342, 'health': 0.9587749604699763}\n",
      "=============================================\n",
      "now processing file: primary-tumor.csv ****************\n",
      "---------------------------------------------\n",
      "missing value for each column [0, 1, 67, 155, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "number of possible labels = 21\n",
      "\n",
      "IG *\n",
      "{'age': 0.15474214188705915, 'sex': 0.33536005150555503, 'histologic-type': 1.026223426546728, 'degree-of-diffe': 2.0947714990558133, 'bone': 0.21246189904816637, 'bone-marrow': 0.020366938848048743, 'lung': 0.10088123982399066, 'pleura': 0.06787277570442418, 'peritoneum': 0.220521934706706, 'liver': 0.19976143639025246, 'brain': 0.06714460241010611, 'skin': 0.06025390884525317, 'neck': 0.29153013602249356, 'supraclavicular': 0.12715354518198296, 'axillar': 0.2458886814337724, 'mediastinum': 0.18425767171538476, 'abdominal': 0.1701481108388725}\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "for i in files:\n",
    "    print(\"now processing file:\",i,\"****************\")\n",
    "    #test = naive_bayes_learner(datapath, i)\n",
    "    #test.run(v='random sampling')\n",
    "    #print(\"---------------------------------------------\")\n",
    "    test = naive_bayes_learner(datapath, i)\n",
    "    test.run(v='hold-out')\n",
    "    print(\"---------------------------------------------\")\n",
    "    test = naive_bayes_learner(datapath, i)\n",
    "    test.run(v='full', ig=True, mi=True)\n",
    "    print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing file: anneal.csv ****************\n",
      "[0.97, 1.0, 0.98, 0.99, 0.99, 0.98, 1.0, 0.99, 0.98, 0.98]\n",
      "[0.98, 0.99, 0.98, 0.97, 0.99, 0.99, 0.99, 0.98, 0.97, 0.99]\n",
      "[0.98, 0.98, 0.98, 0.98, 0.99, 0.97, 0.99, 0.99, 0.99, 0.98]\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 5\n",
      "now processing file: breast-cancer.csv ****************\n",
      "[0.75, 0.75, 0.82, 0.68, 0.64, 0.57, 0.68, 0.82, 0.75, 0.71]\n",
      "[0.71, 0.77, 0.72, 0.7, 0.72, 0.78, 0.73, 0.67, 0.69, 0.72]\n",
      "[0.7, 0.73, 0.71, 0.67, 0.73, 0.7, 0.73, 0.74, 0.74, 0.7]\n",
      "missing value for each column [0, 0, 0, 0, 8, 0, 0, 1, 0, 0]\n",
      "number of possible labels = 2\n",
      "now processing file: car.csv ****************\n",
      "[0.83, 0.87, 0.86, 0.87, 0.84, 0.86, 0.84, 0.83, 0.83, 0.86]\n",
      "[0.87, 0.86, 0.84, 0.86, 0.85, 0.84, 0.84, 0.85, 0.86, 0.85]\n",
      "[0.87, 0.85, 0.86, 0.86, 0.85, 0.85, 0.86, 0.84, 0.85, 0.83]\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 4\n",
      "now processing file: cmc.csv ****************\n",
      "[0.49, 0.49, 0.45, 0.44, 0.54, 0.5, 0.47, 0.49, 0.5, 0.49]\n",
      "[0.51, 0.52, 0.53, 0.53, 0.52, 0.48, 0.47, 0.52, 0.51, 0.49]\n",
      "[0.48, 0.49, 0.49, 0.48, 0.48, 0.5, 0.49, 0.47, 0.5, 0.48]\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 3\n",
      "now processing file: hepatitis.csv ****************\n",
      "[0.93, 0.87, 1.0, 0.67, 0.87, 0.6, 0.67, 0.73, 0.87, 0.87]\n",
      "[0.81, 0.87, 0.79, 0.91, 0.89, 0.79, 0.87, 0.83, 0.87, 0.89]\n",
      "[0.83, 0.85, 0.83, 0.82, 0.86, 0.88, 0.87, 0.86, 0.85, 0.86]\n",
      "missing value for each column [0, 1, 0, 1, 1, 1, 10, 11, 5, 5, 5, 5, 0, 0]\n",
      "number of possible labels = 2\n",
      "now processing file: hypothyroid.csv ****************\n",
      "[0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n",
      "[0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n",
      "[0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n",
      "missing value for each column [73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 2\n",
      "now processing file: mushroom.csv ****************\n",
      "[0.96, 0.96, 0.97, 0.97, 0.98, 0.97, 0.97, 0.97, 0.97, 0.97]\n",
      "[0.97, 0.97, 0.96, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97]\n",
      "[0.97, 0.98, 0.98, 0.98, 0.96, 0.97, 0.97, 0.97, 0.97, 0.97]\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2480, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 2\n",
      "now processing file: nursery.csv ****************\n",
      "[0.89, 0.9, 0.9, 0.9, 0.91, 0.91, 0.9, 0.9, 0.9, 0.91]\n",
      "[0.91, 0.91, 0.9, 0.9, 0.9, 0.9, 0.91, 0.9, 0.91, 0.9]\n",
      "[0.91, 0.9, 0.9, 0.9, 0.9, 0.9, 0.91, 0.91, 0.9, 0.91]\n",
      "missing value for each column [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of possible labels = 5\n",
      "now processing file: primary-tumor.csv ****************\n",
      "[0.42, 0.42, 0.48, 0.64, 0.45, 0.61, 0.52, 0.48, 0.42, 0.42]\n",
      "[0.43, 0.52, 0.47, 0.41, 0.49, 0.51, 0.47, 0.48, 0.42, 0.47]\n",
      "[0.45, 0.46, 0.46, 0.45, 0.35, 0.45, 0.45, 0.4, 0.45, 0.4]\n",
      "missing value for each column [0, 1, 67, 155, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "number of possible labels = 21\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame()\n",
    "\n",
    "for f in files:\n",
    "    print(\"now processing file:\",f,\"****************\")\n",
    "    h1 = []\n",
    "    h3 = []\n",
    "    h5 = []\n",
    "    r = 0\n",
    "    for i in range(10):\n",
    "        test = naive_bayes_learner(datapath, f)\n",
    "        test.run(v='hold-out',h=0.1)\n",
    "        h1.append(test.acc)\n",
    "        test = naive_bayes_learner(datapath, f)\n",
    "        test.run(v='hold-out',h=0.3)\n",
    "        h3.append(test.acc)\n",
    "        test = naive_bayes_learner(datapath, f)\n",
    "        test.run(v='hold-out',h=0.5)\n",
    "        h5.append(test.acc)\n",
    "        \n",
    "    print([round(j,2) for j in h1])\n",
    "    print([round(j,2) for j in h3])\n",
    "    print([round(j,2) for j in h5])\n",
    "    test = naive_bayes_learner(datapath, f)\n",
    "    test.run(v='random sampling')\n",
    "    r = test.acc\n",
    "    test = naive_bayes_learner(datapath, f)\n",
    "    test.run(v='full', mi=True)\n",
    "    summary[f] = pd.Series([r,(round(min(h1),2), round(max(h1),2)), (round(min(h3),2), round(max(h3),2)),(round(min(h5),2), round(max(h5),2)),test.acc, len(test.labels), test.df.shape[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.index = ['Random Sampling', '9-1 Hold-out', '7-3 Hold-out', '5-5 Hold-out','Full', '# of labels', '# of instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anneal.csv</th>\n",
       "      <th>breast-cancer.csv</th>\n",
       "      <th>car.csv</th>\n",
       "      <th>cmc.csv</th>\n",
       "      <th>hepatitis.csv</th>\n",
       "      <th>hypothyroid.csv</th>\n",
       "      <th>mushroom.csv</th>\n",
       "      <th>nursery.csv</th>\n",
       "      <th>primary-tumor.csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Sampling</th>\n",
       "      <td>0.96648</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.445578</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.954114</td>\n",
       "      <td>0.971059</td>\n",
       "      <td>0.907793</td>\n",
       "      <td>0.58209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9-1 Hold-out</th>\n",
       "      <td>(0.97, 1.0)</td>\n",
       "      <td>(0.57, 0.82)</td>\n",
       "      <td>(0.83, 0.87)</td>\n",
       "      <td>(0.44, 0.54)</td>\n",
       "      <td>(0.6, 1.0)</td>\n",
       "      <td>(0.95, 0.95)</td>\n",
       "      <td>(0.96, 0.98)</td>\n",
       "      <td>(0.89, 0.91)</td>\n",
       "      <td>(0.42, 0.64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7-3 Hold-out</th>\n",
       "      <td>(0.97, 0.99)</td>\n",
       "      <td>(0.67, 0.78)</td>\n",
       "      <td>(0.84, 0.87)</td>\n",
       "      <td>(0.47, 0.53)</td>\n",
       "      <td>(0.79, 0.91)</td>\n",
       "      <td>(0.95, 0.95)</td>\n",
       "      <td>(0.96, 0.97)</td>\n",
       "      <td>(0.9, 0.91)</td>\n",
       "      <td>(0.41, 0.52)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-5 Hold-out</th>\n",
       "      <td>(0.97, 0.99)</td>\n",
       "      <td>(0.67, 0.74)</td>\n",
       "      <td>(0.83, 0.87)</td>\n",
       "      <td>(0.47, 0.5)</td>\n",
       "      <td>(0.82, 0.88)</td>\n",
       "      <td>(0.95, 0.95)</td>\n",
       "      <td>(0.96, 0.98)</td>\n",
       "      <td>(0.9, 0.91)</td>\n",
       "      <td>(0.35, 0.46)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.758741</td>\n",
       "      <td>0.873843</td>\n",
       "      <td>0.505771</td>\n",
       "      <td>0.83871</td>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.969842</td>\n",
       "      <td>0.903086</td>\n",
       "      <td>0.60177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># of labels</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># of instance</th>\n",
       "      <td>898</td>\n",
       "      <td>286</td>\n",
       "      <td>1728</td>\n",
       "      <td>1473</td>\n",
       "      <td>155</td>\n",
       "      <td>3163</td>\n",
       "      <td>8124</td>\n",
       "      <td>12960</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   anneal.csv breast-cancer.csv       car.csv       cmc.csv  \\\n",
       "Random Sampling       0.96648          0.666667      0.869565      0.445578   \n",
       "9-1 Hold-out      (0.97, 1.0)      (0.57, 0.82)  (0.83, 0.87)  (0.44, 0.54)   \n",
       "7-3 Hold-out     (0.97, 0.99)      (0.67, 0.78)  (0.84, 0.87)  (0.47, 0.53)   \n",
       "5-5 Hold-out     (0.97, 0.99)      (0.67, 0.74)  (0.83, 0.87)   (0.47, 0.5)   \n",
       "Full                 0.989978          0.758741      0.873843      0.505771   \n",
       "# of labels                 5                 2             4             3   \n",
       "# of instance             898               286          1728          1473   \n",
       "\n",
       "                hepatitis.csv hypothyroid.csv  mushroom.csv   nursery.csv  \\\n",
       "Random Sampling      0.774194        0.954114      0.971059      0.907793   \n",
       "9-1 Hold-out       (0.6, 1.0)    (0.95, 0.95)  (0.96, 0.98)  (0.89, 0.91)   \n",
       "7-3 Hold-out     (0.79, 0.91)    (0.95, 0.95)  (0.96, 0.97)   (0.9, 0.91)   \n",
       "5-5 Hold-out     (0.82, 0.88)    (0.95, 0.95)  (0.96, 0.98)   (0.9, 0.91)   \n",
       "Full                  0.83871        0.952261      0.969842      0.903086   \n",
       "# of labels                 2               2             2             5   \n",
       "# of instance             155            3163          8124         12960   \n",
       "\n",
       "                primary-tumor.csv  \n",
       "Random Sampling           0.58209  \n",
       "9-1 Hold-out         (0.42, 0.64)  \n",
       "7-3 Hold-out         (0.41, 0.52)  \n",
       "5-5 Hold-out         (0.35, 0.46)  \n",
       "Full                      0.60177  \n",
       "# of labels                    21  \n",
       "# of instance                 339  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Question 1 and 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question 1\n",
    "  - Based on the code results which I printed out above, IG tends to prefer dataset which has an unevenly distributed IG (large variance). For a single dataset, higher IG trends to affect the final decision (decide which class does the instance belongs to) more. For example, in dataset nursery.csv, attribute 'health' has a extremely high IG which boost the decision making for high accuacy. On the other hand, for evenly large IG, it will give a relatively bad prediction which refer to the CMC.csv.\n",
    "  - **Strange Results**\n",
    "    - For dataset hypothyroid.csv, although it has a relatively low IG but the prediction has a pretty high accuracy.\n",
    "        - Since the IG only consider how good is a single attribute when making decison. However, NB tring to combine several independent attributes to make the final decison. Therefore, the IG is low but acc remains high.\n",
    "    - For dataset primary-tumor.csv, it has two very high IG but has a low acc.\n",
    "        - The reason for that is it contain a huge proportion of missing values for those two features. \n",
    "- Question 4\n",
    "  - I've implemented the following evaluation method\n",
    "    1. Full-test and full-train\n",
    "    2. Hold-out (by default, 8-2 split, but tested on different splits (9-1, 7-3, 5-5) as well distributions are kept)\n",
    "    3. random Sampling (by default, 8-2 split, distributions are not kept)\n",
    "  - When comparing method 1 and 2, for dataset\n",
    "    - For most of the dataset, the acc for different methods are in the similar range. Those datasets are large enough to learn the pattern within the datasets. As we keep the distribution of data when extracting training data from the dataset for method 2, the performance (acc) are hardly be affected.\n",
    "    - For samll dataset, the biased data affects 9-1 hold-out more. （as I illustrated the data above, 9-1 hold-out normally has a very large range for small and biased dataset）\n",
    "        - It is not vary hard to understand, since there are only few testing data, one outlier will result in significatly change in acc.\n",
    "    - Specific dataset yield some interesting results\n",
    "      - Breast-cancer and hepatitis\n",
    "        - Although they got quite similar results in acc. It seems varying in a quite large range for method 2. One of the reason is that small number of instance we have. As the distributions are kept, another reason for this is that the data are biased. The reason has been explained above.\n",
    "      - Primary-tumor\n",
    "        - For this dataset, missing values take a huge count in significant larger acc of method 1(refer to the missing value that I printed out for each attribute above). Lacking of instance also becomes a problem here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
