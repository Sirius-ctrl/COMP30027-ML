\documentclass[11pt]{article}
\author{anonymous}
\usepackage{colacl}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{url}
\sloppy



\title{COMP30027 Report}



\begin{document}
\maketitle


%\begin{abstract}
%Don't include an abstract.
%\end{abstract}

\section{Introduction}
This project is about to pick suitable machine learning strategy to
predict the geotag based on the text generated by user. In this
project, I will discuss my work from the following perspective:
data pre-precessing[\ref{sec2}], Models[\ref{models}] that I explored.

\section{Data Preprocessing}
\label{sec2}
When working on this project, I found out that it
is really important to have a good tool to transfer the plain text
to useful and suitable new features to feed into the model.
Better data preprocessing strategy may produce more useful information 
and more choice for our later feature engineering.

\subsection{*-top* Files Only}
Those files were originally provided to us. Even though those words
are highly correlated (in terms of all the words) to our labels, 
whereas, up to 78\% of the
instances contain only 0 for all the attributes. If I could 
correctly predict over 70\% of the non-zero labels data and use 
0-R to assign the others to one label, 34.9\%
\footnote{$0.7*0.22+0.25*0.78$} accuracy could be 
reached overall. However, based on my discovery, it is extremely
hard to reach even 50\% on the non-zero part. Also, the real world
data wouldn't be evenly distributed. In this case, the 
data could not be very useful. Extra work is required.

\subsection{Raw File Text Precessing}
Another choice becomes to use the raw tweets and process it to
usable data. 

\subsubsection{Step 1 - Split Word By Space}
Plain text is hard to use but words are easier. Therefore, the first
step is to split the text by space.

\subsubsection{Step 2 - Reduce Redundancy}
Although we get some usable words data, there are still a lot of
noise and redundancy in it.
The primary thing to do is to remove all the punctuation
which are widely used but uninformative.
After that, by sampling some text from the tweets, I notice that the 
word after
\# and \@ might be extremely useful as they may contains information 
hight correlated to one of the label but rarely occur, 
for instance, "\@BrisbaneNews9". What I did for string like this is
that I split them into smaller words by capital letter. Notice that
I might get a text like this "\#LouisWhyAreYouAnEGG", the EGG at the
end won't become ["E","G", "G"]. Instead, it will becomes a whole word
"EGG". After this process, all words are transfer to lowercase to 
prevent that people using words in unexpected way.

\subsubsection{Step 3 - Reduce Sample Size}
Up until now, the sample size are huge, throwing some less 
informative words is necessary. In this steps, most
\footnote{more will be removed in later section} of stop words
and unicode emoji are removed.
Those words are widely used but less informative.
Roughly 25\% features are removed in this step.


\section{Feature Engineering}
\label{ext}
Feature engineering is another very important part.
Based on the performance of my pre-processing strategy, 
same set of data gain a 5\% 
\footnote{figure obtained by 20-fold CV on train-raw and dev-raw 
by using MNB and Logistic regression(Saga)}
boost in accuracy when comparing to using train-top100.
In this section, I will discuss two feature engineering approach that
I used. There performance will be mentioned in model section.
[\ref{models}]

\subsection{Frequency Based}
Since the feature set are still very large, but number of instance
are relative small. To avoid overfitting and variance of our model, 
I prefer to take the top 5000
\footnote{2000-10000 were tested with steps=500, 5000 is reasonably good
for my model}
most frequent words among all tweets. 
Two difference representations for this method are
used, the first one is one-hot (preferred by most of the model) stored in 
sparse matrices, the other one use the number of ranking in frequency 
(start from 1) and then use 
0 padding to extend to the same length (preferred by word embedding).

\subsection{TF-IDF}
TF-IDF is a commonly used feature selection tools in NLP, it measure
the how important is a word from the documents perspective. It stands for
term frequency(TF) and inverse document frequency(IDF).
The TF part measure how frequent we could see a word in a document, 
IDF represent the incorporated which diminishes the weight of terms that 
occur very frequently in the document set and increases the 
weight of terms that occur rarely.(\newcite{wiki:tfidf})
By using this method, the words in those tweets could be weighted
properly. What should not be neglected is that I process the text both
in word level and char level (with n-gram range 2-6 for word to char)
\footnote{optimal hyper-parameters found by loop through multiple
possible combinations}. The rest of the stop words are removed at this
stage. Finally, there are 60,000 feature created in total. This feature 
engineering method produce the best accuracy among all the method that
I tried.

\section{Models}
\label{models}
In this project, I considered both deep learning approaches and 
classical statistical machine learning methods and becomes my final
choice.

Table1[\ref{accuracy}] is
the accuracy for all the model I tried (CV represent the mean accuracy
yield by implementing 5 or 10 fold cross-validation and new represent 
accuracy tested on dev set)
I use the results of cv to fine-tune the hyper-parameters of my model 
,then test it on dev set to figure out how well are my models perform
on relatively large unseen data. The reason that I don't use dev set for
testing primarily is because I prefer dev set as real testing set which 
could provide me an opportunity to test my model on brand new data. If 
we only test our data on dev set, we are still trying to learn the
hyper-parameters from dev set which will cause over-estimate.

\begin{table}[h]
  \begin{center}
 \begin{tabular}{|l|l|l|}
 
       \hline
       Model & cv 
       & new\\
       \hline\hline
       MNB & 42\% & 34.3\% \\
       Logistic Regression & 40.1\% & 34.1\% \\
       Random Forest & 40.2\% & 34.1\% \\
       Linear SVM & 40.4\% & 33.1\% \\
       Voting & 40.6\% & 34.7\% \\
       Dense NN & 38\% & 30.8\% \\
       Word Embedding & 60\% & 31.5\% \\
       \hline
 
 \end{tabular}
 \caption{Comparison of accuracy for different models}\label{accuracy}
  \end{center}
 \end{table}


\subsection{Classical Statistical Approach}

Initially, I prefer these approaches to be baseline, 
whereas, they produce a lot of outstanding results and becomes my 
final choice.

\subsubsection{Multinomial Naive Bayes (sklearn)}
\begin{itemize}
      \item 
            NB is normally the first approach people 
            want to try when doing a NLP task. Also, It could handle multi-class
            problems naturally and yield some decent result. Since we know this
            is a multi-class prediction and the data distribution follows a multinomial
            distribution, we chose the MNB classifier.
\end{itemize}

\subsubsection{Logistic Regression (sklearn)}
\begin{itemize}
      \item 
            sag/saga solver works pretty well on 
            multi-class problems and perform fast convergence in problem with huge data set.

      \item Only change solver to saga for the purpose of saving memory
            (compare to newton's method) Hessian matrices are not required, 
            convergence grantee for enough training instances (compare to 
            quasi-newton's method),
            support multi-class instead of one-vs-rest (compare to linear solver) 
            and support more normalization method (compare to sag).
\end{itemize}

\subsubsection{Random Forest (sklearn)}
\begin{itemize}
      \item Why this model was chosen: Ensemble learning normally works well and 
            it could handle multi-class problem naturally. In Random Forest, the
            variance of the model could be minimized.
\end{itemize}

\subsubsection{LinearSVM (sklearn)}
Why this model was chosen (\newcite{linearkernal}):
\begin{enumerate}
      \item most text are linear separable
      \item SVM has works well on a lot of different jobs with good interpretability
      \item Normal NLP task contains a huge number of instance and features.
            Since documents contains a lot of unique words and we are trying to
            map the features to high dimension. In this case, using linear
            kernel could significantly improve our training time.
      \item Less work in fine-tuning, since only regularization parameter.
\end{enumerate}

\subsubsection{Voting (sklearn)}
\begin{itemize}
      \item Voting is a robust learning method, works well on multiple weak learner.
      \item 
\end{itemize}

\subsection{Deep Learning}
Deep learning has some advantages comparing to non-deep learning, which require less
feature engineering but more on architectural engineering. It could help the machine 
learner to do some feature selection as it is really good at fitting the data.

\subsubsection{Feedforward Neural Network (Keras+Tensorflow)}
\begin{itemize}
      \item Why this model was chosen:
\end{itemize}

\subsubsection{Word Embedding (Keras+Tensorflow)}
\begin{itemize}
      \item Why this model was chosen:
\end{itemize}


\section{Conclusions}

In conclusion,

\bibliographystyle{acl}
\bibliography{sample}

\end{document}
